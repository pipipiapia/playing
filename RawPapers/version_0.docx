Fairness Model for recommendation
Insert Subtitle Here
 
Zhang shaochun†
 Department Name
 Institution/University Name
 City State Country
 email@email.com
FirstName Surname
 Department Name
 Institution/University Name
 City State Country
 email@email.com
FirstName Surname
 Department Name
 Institution/University Name
 City State Country
 email@email.com
 

 
ABSTRACT
Recommendation system is widely used in the industry (e.g. news, products, music, video, ad). Rankings are the primary interface through which recommendation system provides users with relevant materials. In these two-sided markets, ranking not only affects the utility of the users, but also determines the utility of the item providers. It has already been noted that if the ranking model only optimizes the utility to the users –- as done by all classic algorithms -- can be unfair to the item providers. Because, these ranking often ignore the fairness risk--the bias between item allocation and actual user preferences. With the long-term development of the system, it will cause users' interest to become increasingly sparse. At the same time, the sparse interest in turn leads to more similar items. This risk is becoming more and more harmful to the recommendation system.
In this paper, we propose a novel solution to control the fairness of ranking by offer a new regularization explicitly. This solution can not only be used in the point-wise model, but also in Learning-to-rank. We have applied this solution to taobao live recommendation system. Practice has proved that this solution significantly improves the fairness of recommendation results.
CCS CONCEPTS
• Information systems → Learning to rank. 
KEYWORDS
ranking, rerank, learning-to-rank,  fairness,  bias,  exposure bias,
ACM Reference format:
FirstName Surname, FirstName Surname and FirstName Surname. 2018. Insert Your Title Here: Insert Subtitle Here. In Proceedings of ACM Woodstock conference (WOODSTOCK’18). ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/1234567890
1 INTRODUCTION
In general, the recommended candidates are diverse, and each kind of item contributes differently to the system; reciprocally, the user's preference for each interest varies. The recommendation algorithm tends to rank the most relevant results at the top of the result list based on the user's interest, while the less relevant ones are ranked at the middle and bottom. The top material gets more exposure and is reinforced by user feedback, resulting in the phenomenon of "the rich getting richer", which is the Matthew effect.

一般情况下，推荐的候选内容是多种多样的，而且每一类型的内容对于系统的重要程度是不同的；对等情况，用户的兴趣也是如此，用户对每一种类别的兴趣相关性是不同的。推荐算法往往根据用户的兴趣会把最相关的结果排在结果集的头部，而不太相关的排在后面，而后者很大概率得不到曝光。这时，推荐就会存在不公平现象：不太相关的内容因为排在了后面从而再也得不到曝光的机会，相反排在前面的结果由于经过曝光后用户反馈更进一步得到强化，进而造成"富者越富"的现象。

再者，用户对内容的兴趣也是实时动态变化的，这就需要推荐算法能根据用户的实时反馈进行调整，保证推荐结果符合用户的相关。公平推荐模型的目标不仅追求的是实时排序结果的”相关性”，而且要保证推荐结果的“公平性”。
In addition, users' interest changes dynamically in real time, which requires the recommendation algorithm to adjust according to users' real-time feedback to ensure that the recommendation results meet the new needs of users. The target of fair recommendation model is to pursue both the "relevance" of real-time ranking results and the "fairness" of recommendation results distribution.
在此之前有一些研究推荐公平性相关的课题，但这些都面临着一些新的挑战。
In the past, some solutions of fairness control  in recommender systems have not been able to solve the following problems perfectly.

首先，复杂性。研究人员通过单一目标对公平性提出优化[31]，并在指标上有显著提升[13,17,19,23]，但对推荐系统的整体考虑较少。这主要是因为推荐系统本身具有高复杂性，必须由多个模型组成，必须在多个目标之间做取舍[36,50],同时用户的反馈又呈现出稀疏性[3]、动态性和偏差[33]。这些很难解决的问题对公平性优化带来巨大的挑战。
Firstly, complexity. In general, through the single target of optimizing fairness [31], many studies have significantly improved [13,17,19,23]. But due to the neglect of other targets, the overall gain is not significant. This is mainly because the recommendation system itself has high complexity, must be composed of multiple models, and must balance many target [36,50]. At the same time, the feedback data of users are sparse [3], dynamic and biased [33]. All of these problems, which are difficult to solve, pose great challenges to fairness optimization.

其次，动态性。不管是用户的兴趣还是内容的供给都是实时变化的，这是推荐系统中典型的问题。在不同的时间，不同的推荐场景，甚至不同的用户session中，用户的兴趣会呈现不同程度的转移。上一个session对一种东西感觉兴趣，消费的多，然而同样的东西在下一个session就不想再消费了，这就说明用户的兴趣已经发生了转移。推荐本身是基于用户的历史来预测未来[3]，这就导致推荐结果在时间维度上有了偏见。尽管有大量的工作致力于此[42,43]，但是这些问题无疑会加大公平性的难度。
Secondly, dynamism. Both the user's interests and the supplied items usually change in real time, which is a typical problem in recommendation systems. At different times, or in different scenarios, or even in different sessions, the user's interest preferences will show different degrees of evolution. Items that is particularly preferred in this session may be completely uninteresting in the next session. Recommendation models that use users' historical preferences to predict the future [3] can result in a bias between recommendation results and current user preferences. Although a large amount of work has been devoted to this [42,43], these issues undoubtedly make fairness optimization more difficult.

再次，通用性。我们知道，排序模型一般产都首先优化精排模型（pointwise），然后再对头部的结果以list的形式呈现[16,25,34,36,38]时进行ltr(listwise)优化，不然的话，这两个阶段会有明显的gap。但公平性优化就需要同时优化精排模型和ltr模型（同时支持pointwise,listwise)，才能达到整体最优。 这就需要公平性模型具有通用性，能支持多样形式的模型优化。
Thirdly, generalization. Recommendation generally includes two steps: ranking and reranking. The former is pointwise model, to predict pair <user, item >, while the latter is learn-to-rank(ltr), to rank list < Item1, Item2... >.  Actually, only by optimizing the two parts can we get the maximum benefits. Recently, some researchers developed fairness metrics centered around pointwise accuracy [9, 49],  and other researchers offer fairness regulation for ltr model[1]. But, none of them unifies the fairness optimization methods of both parts, so it is impossible to maximize the overall gain.
最后，灵活性。一般情况下公平性和转化效率不能兼得，需要做合理的折中方案。比如过分强调公平性可能会使推荐结果的点击率或者时长降低，因此公平性在不同的场景、不同的时间以及不同程度的活跃用户中进行灵活的配置，以达到公平性和转化效率双目标的合理分配。
Finally, flexibility. Usually, fairness and other targets cannot be achieved the best benefits at the same time, so we need to balance each target flexibly according to the actual needs. For example, over-emphasis on fairness may reduce the click-through rate or the viewing time of recommendation results. Therefore, fairness needs to be reasonably adjusted in different scenarios, at different times, and with different active users, to make a good trade-off between fairness and other goals.

针对以上问题，本文提出了通用的公平性解决方案。通过采集用户的实时反馈信息，对用户的偏好进行无偏估计，进而计算出待排序的物料在公平上的损失，并进行模型优化，最终让未来的推荐结果更符合用户的兴趣分布，即实现公平性。综上所述我们做出的贡献如下：
Addressing all the above problems, this paper proposes a generic fairness improvement solution. By collecting real-time feedback information, an unbiased estimation of users' interests can be performed, and then a fairness loss can be calculated. The fairness loss is added to the objective function during model training to make the recommended results more satisfying to the user's preferences, thus achieving fairness.
We test this on taobao live recommender system in production and show the practical benefits. In summary, we make the following contributions:

•公平性定义:我们推荐的物料符合用户的偏好分布，即明确我们的优化目标。

•用户偏好刻画:我们通过对用户历史和实时的反馈，构建用户的偏好画像。

•正则化:我们提供一种正则化对模型的损失进行修正，以提高模型的公平性指标，同时这种方法通用于point-wise, list-wise。

•现实世界的验证:我们在淘宝直播场景中进行了实验，在公平性方面生成了显著改进，时同也对转化起到正向的促进作用。

2 RELATED WORK
排序模型。以相关性为基础的排序算法已经在当前工业界广泛应用并取得了显示成就，包括搜索引擎、广告、新闻推荐和电商平台。There are several adverse consequences of naive ranking systems [20], such as rich-get-richer, political polarization [11], unfair allocation of exposure [43], and biased judgment [8] through phenomena such as the Matthew effect [3, 24].  这种现象表明，从整体考虑，排序模型不能一味的追求转化效率，更要保证推荐结果的公平性，同时满足用户和内容提供者双边的共同诉求。In this work, 我们在总结前人的基础上，通过对用户偏好的实时无偏估计对推荐结果进行约束，让推荐结果更满足公平性。

用户偏好无偏估计。传统的推荐模型多数以用户的兴趣为基础推荐相关结果，但当一个用户在消费这些内容时，他的行为概率（比如ctr）是实时变化的。当感兴趣的同类内容被过多的展现后，用户开始感觉乏味，消息意愿就会下降（ctr降低），这其中的主要原因就是“用户的行为偏好”和“推荐结果”就会出现偏差。为了降低这种偏差，也就是提高公平性，需要我们对用户的实时行为进行精准跟踪并对偏好进行无偏估计【】。我们也采用了xx等人提出的用户偏好的思路，同时在此基础上进行扩展，由偏好有一维扩展到了N维度。

机器学习的公平。机器学习公平性社区主要关注分类中的公平性，并提出了无数的定义[13,17,19,23]。基于群体公平的定义，即一个模型对两组例子的处理进行比较，已经成为最普遍的结构，但即便如此，研究人员也显示了不同定义之间的紧张关系[31,40]。我们主要遵循Hardt等人[23]的机会均等直觉，我们关注的是不同群体之间的准确性差异。我们的度量最密切地建立在Dixon等人[18]提出的基于aucs的分类和回归公平性度量上，并在[12]中扩展为不同的Mann-Whitney u -test。

推荐系统的公平性。关于排名和推荐的公平性，已经做了少量的工作，但每一项工作都采取了显著不同的视角。Zehlike等人的[52]列出了公平排名的目标，但没有涉及到数据稀少得多的推荐系统。类似地，Singh和joacims[44]对公平性进行了全面排名，但他们能够通过模型预测的后处理算法将其应用于推荐系统;后续工作[45]将此工作转移到模型培训中。所有这些工作[11,44,45,52]关注的是一个非个性化的信息检索设置，其中每个条目都有已知的相关标签;我们专注于个性化推荐——在这方面必须处理数据的稀缺性和偏差。相比之下，[9,49]关注协同过滤的逐点精度不同，但没有将这些指标与最终排名联系起来。

更遥远的是关于推荐者的统计平价的研究，该研究认为，在一些应用程序中，条目应该在[55]组中以相同的速率显示。多样性[32,39,46,52]，过滤气泡[4]，反馈循环[27]，但与机器学习公平性有关，并不是本文的重点。

公平优化。人们提出了许多方法来解决公平问题。后处理可以提供优雅的解决方案[23,44]，但通常需要了解所有例子的群体成员，这在人口统计数据中很少为人所知。相反，在分类器训练过程中，已经开发了许多优化公平性指标的方法，如基于约束的优化[2,22]、对抗学习[8,20,35,37,53,54]和模型预测的正则化[5,7,30,51]。我们在这些正规化方法的基础上，改进了推荐系统的公平性。

3 DYNAMIC USER preferences
在这一节我们详细描述一下什么是用户的动态行为偏好，如何精确的进行量化，这对后面的公平性优化十分重要。

3.1 

3.1 推荐系统
4 THEORETICAL ANALYSIS
5 fairness in pointwise model
6 fairness in ltr
7 CONCLUSION



 
Figure 1: Figure Caption and Image above the caption [In draft mode, Image will not appear on the screen]
Theorem/Proof/Lemma. Insert text here for the enunciation or Math statement. Insert text here for the enunciation or Math statement. Insert text here for the enunciation or Math statement. Insert text here for the enunciation or Math statement. Insert text here for the enunciation or Math statement.
....Insert text here for the Quotation or Extract, Insert text here for the Quotation or Extract, Insert text here for the Quotation or Extract, Insert text here for the Quotation or Extract, Insert text here for the Quotation or Extract, Insert text here for the Quotation or Extract.
1.1 Heading Level 2
In the below paragraph, it is explained how alt-txt value is placed in MS Word 2010. To add alternative text to a picture in Word 2010, follow these steps:
1.	In a Word 2010 document, insert a picture.
2.	Right click on the inserted picture and select the Format Picture option.
3.	Select the Alt Txt option from the left-side panel options.
4.	In the "Title:" and "Description:" text boxes, type the text you want to represent the picture, and then click "Close".
Below are steps to place alt-txt value in MS Word 2013/2016. To add alternative text to a picture in Word 2013/2016, follow these steps:
1.	In a Word 2013/2016 document, insert a picture.
2.	Right click on the inserted picture and select the Format Picture option.
3.	In the settings at the right side of the window, click on the "Layout & Properties" icon (3rd option). 
4.	Expand Alt Txt option.
5.	In the "Title:" and "Description:" text boxes, type the text you want to represent the picture, and then click "Close".
1.1.1 Heading Level 3. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here.
1.1.1.1 Heading Level 4. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here.
ACKNOWLEDGMENTS
Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here. Insert paragraph text here.
REFERENCES
[1]	Patricia S. Abril and Robert Plant, 2007. The patent holder's dilemma: Buy, sell, or troll? Commun. ACM 50, 1 (Jan, 2007), 36-44. DOI: https://doi.org/10.1145/1188913.1188915.
[2]	Sten Andler. 1979. Predicate path expressions. In Proceedings of the 6th. ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages (POPL '79). ACM Press, New York, NY, 226-236. DOI:https://doi.org/10.1145/567752.567774
 [3]	Ian Editor (Ed.). 2007. The title of book one (1st. ed.). The name of the series one, Vol. 9. University of Chicago Press, Chicago. DOI:https://doi.org/10.1007/3-540-09237-4.
[4]	David Kosiur. 2001. Understanding Policy-Based Networking (2nd. ed.). Wiley, New York, NY..

